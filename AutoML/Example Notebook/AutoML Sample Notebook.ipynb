{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "after-illinois",
   "metadata": {},
   "source": [
    "# AWS Marketplace Product Usage Demonstration - Algorithms\n",
    "\n",
    "## Using Algorithm ARN with Amazon SageMaker APIs\n",
    "\n",
    "This sample notebook demonstrates mAdvisor-AutoML algorithm using Algorithm ARN to run training jobs and use that result for inference.\n",
    "\n",
    "## Compatibility\n",
    "This notebook is compatible only with [mAdvisor-AutoML](\"URL\") Algorithm published to AWS Marketplace. \n",
    "\n",
    "***Pre-Requisite:*** Please subscribe to this product before proceeding with this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-citizenship",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-contest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 prefixes\n",
    "common_prefix = \"DEMO-mAdvisor-byo-Monster\"\n",
    "training_input_prefix = common_prefix + \"/training-input-data\"\n",
    "batch_inference_input_prefix = common_prefix + \"/batch-inference-input-data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-strength",
   "metadata": {},
   "source": [
    "### Create the session\n",
    "\n",
    "The session remembers our connection parameters to Amazon SageMaker. We'll use it to perform all of our Amazon SageMaker operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-classification",
   "metadata": {},
   "source": [
    "## Upload the data for training\n",
    "\n",
    "When training large models with huge amounts of data, you'll typically use big data tools, like Amazon Athena, AWS Glue, or Amazon EMR, to create your data in S3. For the purposes of this example, we're using some the classic [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which we have included. \n",
    "\n",
    "We can use use the tools provided by the Amazon SageMaker Python SDK to upload the data to a default bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-johnston",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input = \"Provide S3 Bucket training dataset location\"\n",
    "print (\"Training Data Location \" + training_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-tours",
   "metadata": {},
   "source": [
    "## Creating Training Job using Algorithm ARN\n",
    "\n",
    "Please put in the algorithm arn you want to use below. This can either be an AWS Marketplace algorithm you subscribed to (or) one of the algorithms you created in your own account.\n",
    "\n",
    "The algorithm arn listed below belongs to the [\"mAdvisor-AutoML\"](\"AWS market place URL\") product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from src.scikit_product_arns import ScikitArnProvider\n",
    "\n",
    "algorithm_arn = \"Please Provide Alogorithm ARN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please provide your dataset Target name \n",
    "hyperparameters={\"Target\":\"Target column name\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from sagemaker.algorithm import AlgorithmEstimator\n",
    "\n",
    "algo = AlgorithmEstimator(\n",
    "            algorithm_arn=algorithm_arn,\n",
    "            role=role,\n",
    "            instance_count=1,\n",
    "            instance_type='ml.c4.xlarge',\n",
    "            hyperparameters=hyperparameters,\n",
    "            base_job_name='jobname-from-aws-marketplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-assumption",
   "metadata": {},
   "source": [
    "## Run Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Now run the training job using algorithm arn %s in region %s\" % (algorithm_arn, sagemaker_session.boto_region_name))\n",
    "algo.fit({'train': training_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-pipeline",
   "metadata": {},
   "source": [
    "## Batch Transform Job\n",
    "\n",
    "Now let's use the model built to run a batch inference job and verify it works.\n",
    "\n",
    "### Batch Transform Input Preparation\n",
    "\n",
    "The snippet below is removing the \"label\" column (column indexed at 0) and retaining the rest to be batch transform's input. \n",
    "\n",
    "***NOTE:*** This is the same training data, which is a no-no from a ML science perspective. But the aim of this notebook is to demonstrate how things work end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_input = \"s3 bucket Test dataset location\"\n",
    "print(\"Transform input location \" + transform_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-patio",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = algo.transformer(1, 'ml.m4.xlarge')\n",
    "transformer.transform(transform_input, content_type='text/csv')\n",
    "transformer.wait()\n",
    "\n",
    "print(\"Batch Transform output saved to \" + transformer.output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-munich",
   "metadata": {},
   "source": [
    "#### Inspect the Batch Transform Output in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-discharge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "parsed_url = urlparse(transformer.output_path)\n",
    "bucket_name = parsed_url.netloc\n",
    "print(parsed_url,'p')\n",
    "print(bucket_name,'')\n",
    "file_key = '{}/{}.out'.format(parsed_url.path[1:], \"Test dataset Name ex: iris_test.csv\")\n",
    "\n",
    "s3_client = sagemaker_session.boto_session.client('s3')\n",
    "\n",
    "response = s3_client.get_object(Bucket = sagemaker_session.default_bucket(), Key = file_key)\n",
    "response_bytes = response['Body'].read().decode('utf-8')\n",
    "print(response_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-impossible",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
